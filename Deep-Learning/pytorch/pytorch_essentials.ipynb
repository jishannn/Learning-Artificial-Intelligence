{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**What is a Tensor?**\n",
        "\n",
        "A tensor is just a multi-dimensional array (like NumPy arrays) but optimized for GPUs. They are the building blocks of PyTorch.\n",
        "\n",
        "ðŸ”¹ Scalars (0D Tensor) â†’ Single number\n",
        "\n",
        "ðŸ”¹ Vectors (1D Tensor) â†’ List of numbers\n",
        "\n",
        "ðŸ”¹ Matrices (2D Tensor) â†’ Table of numbers\n",
        "\n",
        "ðŸ”¹ Higher-Dimensional Tensors (3D, 4D, etc.) â†’ Like image/video data"
      ],
      "metadata": {
        "id": "KOmDvbT8hhNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Creating Tensors**:\n",
        "Manually creating tensors"
      ],
      "metadata": {
        "id": "vCY2quy3hwjp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK8GhJjmhQGB"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scalar (0D)\n",
        "\n",
        "scalar = torch.tensor(5)\n",
        "print(scalar, scalar.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gtWJZFgiJB0",
        "outputId": "7e491bae-fbcd-4629-c036-4b5bafaf536e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5) torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector (1D)\n",
        "vector = torch.tensor([1, 2, 3])\n",
        "print(vector, vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPsfDKGoiUV0",
        "outputId": "1a4ef508-c48f-470f-8bfc-0e08ec0aea3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3]) torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix (2D)\n",
        "matrix = torch.tensor([ [1, 2, 3], [4, 5, 6] ])\n",
        "print(matrix, matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et-Y13Ywirb9",
        "outputId": "7e30e608-3857-4376-f6b0-ad29be0a75f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]]) torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3D Tensor (like an RGB with 3 channels)\n",
        "tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
        "print(tensor_3d, tensor_3d.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHWMxhuOi9w0",
        "outputId": "abc644f6-2016-4e03-f005-84bc666c456f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]]) torch.Size([2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 -  Creating Tensors with Special Values**"
      ],
      "metadata": {
        "id": "uwXR5uh5kQbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All zeros\n",
        "zeros = torch.zeros(3, 3)\n",
        "print(zeros)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otiCD0NQjoJ9",
        "outputId": "a6df6d13-684c-4a46-9454-ef1ea79b98f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All ones\n",
        "ones = torch.ones(2, 2)\n",
        "print(ones)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5r3ERMwkkKn",
        "outputId": "8249a3b7-43fc-450b-83bd-b2b131592a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random values\n",
        "random = torch.rand(3, 3)\n",
        "print(random)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRZqdeG0k0jf",
        "outputId": "0b26be65-e760-47c2-a553-84ea0959792c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4193, 0.3982, 0.6006],\n",
            "        [0.5107, 0.9279, 0.6164],\n",
            "        [0.3213, 0.0068, 0.5915]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identity matrix\n",
        "eye = torch.eye(3)\n",
        "print(eye)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD1_lnA4lBxJ",
        "outputId": "da173ccd-5087-425b-f167-9e63d627a3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor with specific data type\n",
        "float_tensor = torch.ones(3, 3, dtype=torch.float32)\n",
        "print(float_tensor.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trd_FJBClMJR",
        "outputId": "430165c2-9da2-4b47-a987-983d099130aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tensor Operations (Math & Indexing)**"
      ],
      "metadata": {
        "id": "f83DlP91mEsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Math Operations"
      ],
      "metadata": {
        "id": "KO4J99kumJau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([2, 3, 4])\n",
        "b = torch.tensor([1, 5, 7])\n",
        "\n",
        "# Element-Wise Operations\n",
        "print(a + b)\n",
        "print(a - b)\n",
        "print(a * b)   # (Element-wise multiplication)\n",
        "print(a / b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA328rzvl1cT",
        "outputId": "88a6f43a-392e-4921-9087-33493c1dac62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 3,  8, 11])\n",
            "tensor([ 1, -2, -3])\n",
            "tensor([ 2, 15, 28])\n",
            "tensor([2.0000, 0.6000, 0.5714])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.tensor([[1, 2], [3, 4]])\n",
        "B = torch.tensor([[5, 6],\n",
        "                  [7, 8]])\n",
        "\n",
        "# Element-Wise multiplication\n",
        "print(A * B)\n",
        "\n",
        "# Matrix multiplication\n",
        "matrix_multiplication = torch.matmul(A, B)\n",
        "print(matrix_multiplication)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvGh2gvlmeLU",
        "outputId": "81cf9aea-dedd-4f96-8085-55342919fc28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5, 12],\n",
            "        [21, 32]])\n",
            "tensor([[19, 22],\n",
            "        [43, 50]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix multiplication is a fundamental operation in linear algebra.\n",
        "\n",
        "\n",
        "matrix_multiplication = torch.matmul(A, B)\n",
        "\n",
        "\n",
        "To calculate the result, you take the rows of the first matrix (A) and multiply them by the columns of the second matrix (B) in a specific way. Here's the breakdown:\n",
        "\n",
        "\n",
        "Result (top-left): (15) + (27) = 19\n",
        "\n",
        "Result (top-right): (16) + (28) = 22\n",
        "\n",
        "Result (bottom-left): (35) + (47) = 43\n",
        "\n",
        "Result (bottom-right): (36) + (48) = 50\n",
        "\n",
        "\n",
        "Therefore, the result of the matrix multiplication is:\n",
        "\n",
        "[[19, 22],\n",
        "\n",
        "  [43, 50]]"
      ],
      "metadata": {
        "id": "JwNos4iqqmTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 - Indexing & Slicing (Like NumPy)**"
      ],
      "metadata": {
        "id": "kz0ElrMxsJn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[10, 20, 30], [40, 50, 60]])\n",
        "\n",
        "# Get single element\n",
        "print(x[0, 1])\n",
        "\n",
        "# Get a row\n",
        "print(x[1])\n",
        "\n",
        "# Get first column\n",
        "print(x[:, 0])\n",
        "\n",
        "# Get sub-matrix\n",
        "print(x[0:2, 1:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJcRFKCEneI2",
        "outputId": "6f65e15b-b151-4298-cbb4-423ba1ed33e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(20)\n",
            "tensor([40, 50, 60])\n",
            "tensor([10, 40])\n",
            "tensor([[20, 30],\n",
            "        [50, 60]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. NumPy â†” PyTorch Conversion**"
      ],
      "metadata": {
        "id": "pkOEV_QVwlvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Convert NumPy Array to PyTorch Tensor"
      ],
      "metadata": {
        "id": "MSaEthvnwnJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "_8rWbnaKsfu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_array = np.array([1, 2, 3])\n",
        "torch_tensor = torch.from_numpy(numpy_array)\n",
        "\n",
        "print(f'numpy array:', numpy_array)\n",
        "print(torch_tensor)\n",
        "print(torch_tensor.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy3lLM9wwvv3",
        "outputId": "652e53c0-b7af-4489-c709-cb8cdf791b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy array: [1 2 3]\n",
            "tensor([1, 2, 3])\n",
            "torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Convert PyTorch Tensor to NumPy"
      ],
      "metadata": {
        "id": "1AiX5ZAcx3pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_np_array = torch_tensor.numpy()\n",
        "print(new_np_array)     # [1 2 3]\n",
        "print(type(new_np_array))    # <class 'numpy.ndarray'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzwb9xKyw1XJ",
        "outputId": "f83107a4-31c6-4acc-d3b5-54d6c7e34fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Tensor Reshaping**"
      ],
      "metadata": {
        "id": "6ZHSsGzeyyj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing Tensor Shape"
      ],
      "metadata": {
        "id": "myXpo-B8y3gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4SRx3yMyX6x",
        "outputId": "0fba203c-d4d9-4188-ac1e-496ac4d6e36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " .view() vs .reshape()"
      ],
      "metadata": {
        "id": "fLCFYMY7zL38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change shape (2x3 â†’ 3x2)\n",
        "x_view = x.view(3, 2)\n",
        "print(f'x_view:', x_view.shape)\n",
        "\n",
        "# Alternative method\n",
        "x_reshape = x.reshape(3, 2)\n",
        "print(f'x_reshape:', x_reshape.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYxfR6_szEDZ",
        "outputId": "1e931716-35f9-4a76-abc5-4e97df65c347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_view: torch.Size([3, 2])\n",
            "x_reshape: torch.Size([3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key difference: **.view()** returns a view of the same memory, while **.reshape()** creates a new copy if needed."
      ],
      "metadata": {
        "id": "vDItVoOC0ZiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**.squeeze() & .unsqueeze()** : These are useful when dealing with single dimensions (1D â†’ 2D or 2D â†’ 1D)."
      ],
      "metadata": {
        "id": "fUhB_f_y0krL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**.squeeze() â†’ Removes dimensions of size 1**"
      ],
      "metadata": {
        "id": "CsH-jtOa051V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[[1, 2, 3]]])    # Shape: [1, 1, 3]\n",
        "print(x.shape)              # Output: torch.Size([1, 1, 3])\n",
        "\n",
        "x_squeezed = x.squeeze()\n",
        "print(x_squeezed.shape)   # Output: torch.Size([3]) (removes [1, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-2AE1m5zSco",
        "outputId": "25bd3786-280f-41e1-fa91-f0e118d84284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 3])\n",
            "torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**.unsqueeze() â†’ Adds a new dimension**"
      ],
      "metadata": {
        "id": "9hMvJuEm1fTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor([1, 2, 3])\n",
        "print(y.shape)\n",
        "\n",
        "y_unsqueezed = y.unsqueeze(0)      # Adds a new batch dimension\n",
        "print(y_unsqueezed.shape)          # Output: torch.Size([1, 3])\n",
        "\n",
        "y_unsqueezed = y.unsqueeze(1)      # Adds a new column dimension\n",
        "print(y_unsqueezed.shape)          # Output: torch.Size([3, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uyq3L_HL0_HQ",
        "outputId": "c908e639-e874-424b-cdb0-91a71b20afc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3])\n",
            "torch.Size([1, 3])\n",
            "torch.Size([3, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Differences and When to Use Them:\n",
        "\n",
        "**squeeze()** removes dimensions of size 1. Use it when you want to simplify a tensor by getting rid of unnecessary single-sized dimensions.\n",
        "\n",
        "**unsqueeze()** adds a dimension of size 1. Use it when you need to add a dimension that is required by a function or model, such as a batch dimension."
      ],
      "metadata": {
        "id": "laf9hAfu4WwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Broadcasting**"
      ],
      "metadata": {
        "id": "IfIA-RUi4ndW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1], [2], [3]])  # Shape: [3, 1]\n",
        "b = torch.tensor([4, 5, 6])        # Shape: [3] -> treated as [1, 3]\n",
        "\n",
        "print(a + b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1Zoa-vV1m4x",
        "outputId": "b1d22297-6a7b-4028-87c1-3650823f2978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5, 6, 7],\n",
            "        [6, 7, 8],\n",
            "        [7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch automatically **expands smaller tensors to match larger ones**."
      ],
      "metadata": {
        "id": "U7Kxt7Dz5A2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Autograd (Automatic Differentiation)**"
      ],
      "metadata": {
        "id": "FNcUpFySNTSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd is a core feature in PyTorch that allows automatic calculation of gradients. This is crucial for training models, as gradients are used to adjust weights during optimization."
      ],
      "metadata": {
        "id": "mOYVrV8wNja4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**requires_grad â€“ Tracking gradients for a tensor**\n",
        "\n",
        "**backward() function - Computing the gradients**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dKCBKAuUOIK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **requires_grad** attribute tells PyTorch to track operations on a tensor so that it can compute gradients later when performing the backward pass.\n",
        "\n",
        "\n",
        "The **backward()** function computes the derivatives (gradients) of tensors that require gradients. These gradients are stored in the .grad attribute of the tensor.\n"
      ],
      "metadata": {
        "id": "PzDVjhIdONOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor with requires_grad=True to track computations on it\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# Perform some operations on it\n",
        "y = x**2 + 3*x + 1       # y = x^2 + 3x + 1\n",
        "\n",
        "print(x.requires_grad)   # True\n",
        "print(y.requires_grad)   # Output: True, because y depends on x\n",
        "\n",
        "# Calculate the sum of y to get a scalar loss\n",
        "loss = y.sum()\n",
        "# backward() needs a scalar value (a single number) to start backpropagation.\n",
        "# y is a vector (a tensor with multiple elements).\n",
        "# Here, we calculate the sum of all the elements in y and store it in the variable loss.  Now, loss is a single number (a scalar), representing the overall \"loss\" or error.\n",
        "\n",
        "\n",
        "\n",
        "# Check the gradient for y that is stored in loss variable\n",
        "loss.backward()\n",
        "\n",
        "# Access gradients\n",
        "print(x.grad)  # Output: tensor([7., 9.]), this is dy/dx for each element of x"
      ],
      "metadata": {
        "id": "ogV9Vu8Z4zwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c797db-3631-4349-d9ec-ac5f9c61d446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "tensor([7., 9.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "note:\n",
        "the **backward()** function, by default, expects the tensor you call it on to be a scalar (a single number)\n",
        "\n",
        "\n",
        "y = x**2 + 3*x + 1 will not be a single value (scalar) if x is a tensor with more than one element.  It will be a tensor of the same shape as x."
      ],
      "metadata": {
        "id": "ZNCOiINrUZk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**detach() â€“ Disconnecting from the computation graph**"
      ],
      "metadata": {
        "id": "gCna5B4hZHrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, you may want to stop a tensor from tracking gradients. This can be useful when you donâ€™t want to compute gradients for a specific tensor (e.g., during inference).\n",
        "\n",
        "The detach() method returns a new tensor that shares data with the original tensor but doesnâ€™t track gradients.\n",
        "\n"
      ],
      "metadata": {
        "id": "aLmRReLxZKnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor with requires_grad=True\n",
        "a = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "# Perform some operations\n",
        "b = a * 2\n",
        "\n",
        "# Now, detach 'b' from the computation graph\n",
        "b_detached = b.detach()\n",
        "\n",
        "\n",
        "# 'b' tracks gradients, but 'b_detached' doesn't\n",
        "print(b.requires_grad)           # True\n",
        "print(b_detached.requires_grad)  # False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TDG7KOZVSO8",
        "outputId": "8a7714b5-3bf5-4967-d588-611d66b60317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.no_grad() â€“ No gradient tracking for inference**"
      ],
      "metadata": {
        "id": "FMnF9ksnZq2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During inference, no need to compute gradients since you're only making predictions. The torch.no_grad() context manager temporarily disables gradient tracking, saving memory and speeding up computations.\n",
        "\n",
        "This is useful during evaluation or testing when we donâ€™t need gradients.\n"
      ],
      "metadata": {
        "id": "oQoSJ8hiahnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor with requires_grad=True\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# Without torch.no_grad(), PyTorch will track operations\n",
        "with torch.no_grad():\n",
        "  y = x**2 + 3*x + 1\n",
        "\n",
        "  print(x.requires_grad)    # True\n",
        "  print(y.requires_grad)    # No gradient tracking here.  Output: False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaZmI64DYqUH",
        "outputId": "e9660959-dda6-4230-fb2d-0474b2607325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NN Module (Neural Networks)**\n",
        "\n",
        "In PyTorch, models are typically built by subclassing torch.nn.Module, which makes it easier to define the network architecture, the forward pass, and optimization routines."
      ],
      "metadata": {
        "id": "O6moav1Vmak1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.nn.Module â€“ Base class for all models**\n",
        "\n",
        "To define a custom model, you subclass torch.nn.Module and implement the forward() method, which defines how input data passes through the network."
      ],
      "metadata": {
        "id": "CD-GgzzOmlO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "WcZpudvLaCQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple feedforward neural network (FFNN)\n",
        "class SimpleNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleNN, self).__init__()\n",
        "\n",
        "    # Define layers (Fully connected layer)\n",
        "    self.fc1 = nn.Linear(2, 4)     # Input: 2 features, Output: 4 features\n",
        "    self.fc2 = nn.Linear(4, 1)     # Input: 4 features, Output: 1 feature\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "     # Forward pass through the network (data flows through layers)\n",
        "    x = torch.relu(self.fc1(x))     # Apply ReLU activation after the first layer\n",
        "    x = self.fc2(x)   # Second layer\n",
        "    return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model = SimpleNN()\n",
        "\n",
        "# Example input (2 features)\n",
        "input_data = torch.tensor([1.0, 2.0])\n",
        "\n",
        "# Get the model's output (forward pass)\n",
        "output = model(input_data)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VArMa-dnrCh",
        "outputId": "32a13fc4-048a-427e-c4a4-805167c5a262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1676], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.nn.Parameter â€“ Defining parameters in the model**\n",
        "\n",
        "\n",
        "torch.nn.Parameter is a special type of tensor used to define learnable parameters (like weights and biases) inside a model. PyTorch automatically tracks these parameters for gradient computation."
      ],
      "metadata": {
        "id": "lLUJTqbgxRv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually defining a learnable parameter\n",
        "class CustomLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomLayer, self).__init__()\n",
        "    self.my_param = nn.Parameter(torch.randn(3, 3))   # 3x3 learnable matrix\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.matmul(x, self.my_param)\n",
        "\n",
        "layer = CustomLayer()\n",
        "print(layer.my_param)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QZ9uVHvpTrX",
        "outputId": "1fbac414-f50c-41aa-ef72-3d2a47b73a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.2248,  0.8450, -0.0895],\n",
            "        [-0.5436,  1.0730, -0.6326],\n",
            "        [-0.2650, -1.7898, -0.8515]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**self.my_param** is a learnable parameter in the model. PyTorch will update this parameter during training."
      ],
      "metadata": {
        "id": "XS82tfgXxL--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.nn.Sequential â€“ Simplified model definition**\n",
        "\n",
        "**nn.Sequential** allows you to build models by stacking layers in a sequential order without having to explicitly define the forward() function."
      ],
      "metadata": {
        "id": "UVR4lmUmy0Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple model using Sequential\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 4),    # Input: 2 features, Output: 4 features\n",
        "    nn.ReLU(),          # ReLU activation\n",
        "    nn.Linear(4, 1)      # Input: 4 features, Output: 1 feature\n",
        ")\n",
        "\n",
        "\n",
        "# Example input\n",
        "input_data = torch.tensor([1.0, 2.0])\n",
        "\n",
        "# Forward pass through the model\n",
        "output = model(input_data)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVGMM7HZu4yz",
        "outputId": "64964c88-1e9d-4f59-d84b-b126c8105a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.4216], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Sequential, the layers are automatically applied in the order you define them. It's a quick way to define simple models."
      ],
      "metadata": {
        "id": "mMttXLzQz184"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.nn.functional â€“ Functional API for operations**\n",
        "\n",
        "torch.nn.functional provides a collection of functions (like activation functions, loss functions, etc.) that donâ€™t create new layers or parameters but can still be used inside forward() function.\n",
        "\n",
        "Example: Using ReLU activation and cross-entropy loss from torch.nn.functional:"
      ],
      "metadata": {
        "id": "Zu6Lsq4w13tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Example input\n",
        "x = torch.tensor([[-1.0, 2.0], [1.0, -2.0]])\n",
        "\n",
        "# Applying ReLU using functional API\n",
        "x_relu = F.relu(x)\n",
        "print(x_relu)\n",
        "\n",
        "# Cross-entropy loss example (for classification)\n",
        "y_true = torch.tensor([0, 1])  # True labels\n",
        "y_pred = torch.tensor([[0.2, 0.8], [0.9, 0.1]])  # Predicted probabilities\n",
        "loss = F.cross_entropy(y_pred, y_true)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzLH8f21yVMg",
        "outputId": "d77fb827-432e-4d5d-dc63-b96886c27de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 2.],\n",
            "        [1., 0.]])\n",
            "tensor(1.1043)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example, F.relu() applies the ReLU activation, and F.cross_entropy() computes the cross-entropy loss.\n",
        "\n"
      ],
      "metadata": {
        "id": "sbZbD9zf2BFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizers** in PyTorch\n",
        "\n",
        "Optimizers are crucial in updating model parameters during training to minimize the loss function. PyTorch provides several optimizers, the most common being SGD (Stochastic Gradient Descent) and Adam."
      ],
      "metadata": {
        "id": "HRWT_rAY5JRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.optim.SGD â€“ Stochastic Gradient Descent**\n",
        "\n",
        "SGD is one of the most commonly used optimization algorithms in machine learning. It updates the parameters by computing the gradient of the loss with respect to the model parameters and moving in the opposite direction of the gradient (i.e., minimizing the loss)."
      ],
      "metadata": {
        "id": "QGLbnEwz5SGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "fLtI9F1l0SDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple model\n",
        "model = nn.Linear(2, 2)  # A simple Linear model\n",
        "\n",
        "# Define a simple loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer: Stochastic Gradient Descent\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)    # learning rate = 0.01\n",
        "\n",
        "# Dummy data\n",
        "data = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "target = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "  # Zero the gradients before the backward pass\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Forward pass: Compute predicted y by passing x to the model\n",
        "  output = model(data)\n",
        "\n",
        "  # Compute the loss\n",
        "  loss = criterion(output, target)\n",
        "\n",
        "  # Backward pass: Compute gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # Update model parameters using optimizer\n",
        "  optimizer.step()\n",
        "\n",
        "  # Print loss every 10 epochs\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch [{epoch+1}/100], Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIkI-Dxt5srQ",
        "outputId": "8e4ddc5b-6c3b-4c42-ac5d-e9bfd8fc82a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 3.3989672660827637\n",
            "Epoch [11/100], Loss: 0.26185113191604614\n",
            "Epoch [21/100], Loss: 0.15574973821640015\n",
            "Epoch [31/100], Loss: 0.1468612104654312\n",
            "Epoch [41/100], Loss: 0.14128848910331726\n",
            "Epoch [51/100], Loss: 0.13601897656917572\n",
            "Epoch [61/100], Loss: 0.13094893097877502\n",
            "Epoch [71/100], Loss: 0.12606796622276306\n",
            "Epoch [81/100], Loss: 0.12136895209550858\n",
            "Epoch [91/100], Loss: 0.11684507131576538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = nn.Linear(2, 2)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "data = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "target = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n",
        "\n",
        "for epoch in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  output = model(data)\n",
        "  loss = criterion(output, target)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch [{epoch+1}/100], Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqVE6-_u7Szo",
        "outputId": "2d9dd61d-0cc4-4c37-8934-1f1b6e450caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 7.031500339508057\n",
            "Epoch [11/100], Loss: 4.196232795715332\n",
            "Epoch [21/100], Loss: 2.24301815032959\n",
            "Epoch [31/100], Loss: 1.0958170890808105\n",
            "Epoch [41/100], Loss: 0.5368722081184387\n",
            "Epoch [51/100], Loss: 0.3110317587852478\n",
            "Epoch [61/100], Loss: 0.23197989165782928\n",
            "Epoch [71/100], Loss: 0.20627081394195557\n",
            "Epoch [81/100], Loss: 0.19760601222515106\n",
            "Epoch [91/100], Loss: 0.19300217926502228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key differences between SGD and Adam:\n",
        "\n",
        "**SGD** uses a fixed learning rate and can sometimes get stuck in local minima.\n",
        "\n",
        "**Adam** adjusts the learning rate per parameter based on moment estimates, which helps in faster convergence and stability."
      ],
      "metadata": {
        "id": "FZ0yCgbvDJOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Rate Scheduling (lr_scheduler)**\n",
        "\n",
        "Learning rate scheduling allows you to adjust the learning rate during training to help optimize convergence. This is important for preventing overfitting and speeding up training after a certain number of epochs.\n",
        "\n",
        "\n",
        "PyTorch provides several learning rate scheduling techniques, such as reducing the learning rate by a factor after a certain number of epochs or based on validation performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "it-BpURpHT4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Define a learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# Training loop with learning rate scheduling\n",
        "for epoch in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  output = model(data)\n",
        "  loss = criterion(output, target)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # Step the scheduler to update the learning rate\n",
        "  scheduler.step()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: [{epoch+1}/100] Loss: {loss.item()} Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "    # get_last_lr() gives you the past learning rate, and get_lr() gives you the future learning rate.\n",
        "    # For most logging and monitoring purposes, get_last_lr() is the more appropriate choice."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnoWyj8XAcZQ",
        "outputId": "4f9e8ce4-651c-4c13-bd4e-dfdf3de0473d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/100] Loss: 1.8784157873596996e-07 Learning Rate: 0.01\n",
            "Epoch: [11/100] Loss: 0.0005542826256714761 Learning Rate: 0.01\n",
            "Epoch: [21/100] Loss: 0.00010649542673490942 Learning Rate: 0.01\n",
            "Epoch: [31/100] Loss: 2.3619895728188567e-06 Learning Rate: 0.001\n",
            "Epoch: [41/100] Loss: 3.98743122786982e-06 Learning Rate: 0.001\n",
            "Epoch: [51/100] Loss: 1.3967067502562713e-07 Learning Rate: 0.001\n",
            "Epoch: [61/100] Loss: 3.4494672718210495e-07 Learning Rate: 0.0001\n",
            "Epoch: [71/100] Loss: 1.9325361222399806e-07 Learning Rate: 0.0001\n",
            "Epoch: [81/100] Loss: 9.005836432152137e-08 Learning Rate: 0.0001\n",
            "Epoch: [91/100] Loss: 5.0339355794903895e-08 Learning Rate: 1e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StepLR(optimizer, step_size, gamma): This scheduler reduces the learning rate by a factor of gamma every step_size epochs.\n",
        "\n",
        "scheduler.step(): Updates the learning rate after each epoch.\n",
        "\n",
        "scheduler.get_lr(): Retrieves the current learning rate.\n"
      ],
      "metadata": {
        "id": "fKeZvhedFQ4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Saving and Loading**\n",
        "\n",
        "In PyTorch, saving and loading models is straightforward using torch.save() and torch.load().\n",
        "\n",
        "**Saving a model (torch.save())**\n",
        "\n",
        "There are two ways to save models:\n",
        "\n",
        "Saving the model state_dict (recommended, only saves the model's learned parameters).\n",
        "Saving the entire model (including structure and parameters)."
      ],
      "metadata": {
        "id": "nBnHdaZWuFnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model state_dict\n",
        "torch.save(model.state_dict(), \"model_state_dict.pth\")\n"
      ],
      "metadata": {
        "id": "TTH9s8dTEbec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading a model (torch.load())**\n",
        "\n",
        "To load the saved model, we first need to initialize the model structure and then load the saved state_dict."
      ],
      "metadata": {
        "id": "F_ooAEYZufFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model (same as when it was saved)\n",
        "model = nn.Linear(2, 2)\n",
        "\n",
        "# Load the saved state_dict\n",
        "model.load_state_dict(torch.load(\"model_state_dict.pth\"))\n",
        "\n",
        "# Set the model to evaluation mode (important for inference)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "ueZDud9huh-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Handling**"
      ],
      "metadata": {
        "id": "m94ohqv8wIfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.utils.data.Dataset & DataLoader\n",
        "\n",
        "**torch.utils.data.Dataset â€“ Base class for datasets**\n",
        "\n",
        "The Dataset class is the starting point for working with any dataset in PyTorch. You subclass Dataset to define how your data is loaded and accessed.\n",
        "\n",
        "The two key methods to implement when subclassing Dataset are:\n",
        "\n",
        "__len__() â€“ Returns the size of the dataset (total number of samples).\n",
        "\n",
        "__getitem__() â€“ Fetches a single sample from the dataset."
      ],
      "metadata": {
        "id": "VnImbqQ5v-1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "tntqKL66vYVL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple custom dataset class\n",
        "class SimpleDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    # Example data (input, target)\n",
        "    self.data = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
        "    self.labels = torch.tensor([0, 1, 0, 1])\n",
        "\n",
        "  def __len__(self):\n",
        "    # Return the number of samples\n",
        "    return len(self.data)\n",
        "    # Return a sample and its label\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx], self.labels[idx]\n",
        "\n",
        "# Create dataset instance\n",
        "dataset = SimpleDataset()\n",
        "\n",
        "# Access data\n",
        "print(len(dataset))   # Output: 4 (number of samples)\n",
        "print(dataset[0])     # Output: (tensor([1, 2]), tensor(0)) (first sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnJ_ELwFve5G",
        "outputId": "cb52cdc4-7cd9-4ea7-b254-ad9637e7e4e5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "(tensor([1, 2]), tensor(0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataLoader â€“ Efficient batching and data shuffling**\n",
        "\n",
        "DataLoader is responsible for batching, shuffling, and loading data in parallel using multiple workers. It can take in a Dataset and return batches of data for training."
      ],
      "metadata": {
        "id": "bCsXDbx5zUoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create DataLoader instance\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for batch_data, batch_labels in dataloader:\n",
        "  print(batch_data, batch_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49cRnztIxcYO",
        "outputId": "663b8b1e-59ea-4bcc-dbc5-6695ae00349c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5, 6],\n",
            "        [1, 2]]) tensor([0, 0])\n",
            "tensor([[3, 4],\n",
            "        [7, 8]]) tensor([1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**batch_size=2**: Means each batch will contain 2 samples.\n",
        "\n",
        "**shuffle=True**: Data will be shuffled before being batched."
      ],
      "metadata": {
        "id": "0r70FRCo0d8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Dataset & collate_fn**\n",
        "\n",
        "Sometimes, you need to create a custom dataset where the standard behavior of Dataset and DataLoader doesn't fit exactly, such as when you need to customize how batches are combined or handle variable-length sequences."
      ],
      "metadata": {
        "id": "N0W63Tuo1--e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Custom Dataset**\n",
        "\n",
        "Letâ€™s say you're working with image data where each image has a different size. You can use the collate_fn to handle custom batching logic."
      ],
      "metadata": {
        "id": "yBmY35RR2DXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cqEPrvwO0Svt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataaset with variable-length data\n",
        "class CustomTextDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    # Example text data with different lengths\n",
        "    self.data = ['hello', 'world', 'this', 'is', 'a', 'custom', 'dataset']\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Convert each word to a tensor of character indices\n",
        "    return torch.tensor([ord(c) for c in self.data[idx]])\n",
        "\n",
        "# Custom collate function to handle padding for variable-length sequences\n",
        "def collate_fn(batch):\n",
        "  # Pad sequences to the same length\n",
        "  max_len = max([len(item) for item in batch])\n",
        "  padded_batch = [torch.cat([item, torch.zeros(max_len - len(item))]) for item in batch]\n",
        "  return torch.stack(padded_batch)\n",
        "\n",
        "\n",
        "# Create the dataset\n",
        "dataset = CustomTextDataset()\n",
        "\n",
        "# Use DataLoader with custom collate_fn\n",
        "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for batch in dataloader:\n",
        "  print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av2qORft2Sqb",
        "outputId": "c54fe379-279c-439e-fbd2-11615e74c5d8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[104., 101., 108., 108., 111.],\n",
            "        [119., 111., 114., 108., 100.]])\n",
            "tensor([[116., 104., 105., 115.],\n",
            "        [105., 115.,   0.,   0.]])\n",
            "tensor([[ 97.,   0.,   0.,   0.,   0.,   0.],\n",
            "        [ 99., 117., 115., 116., 111., 109.]])\n",
            "tensor([[100.,  97., 116.,  97., 115., 101., 116.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, **collate_fn** is used to pad variable-length sequences (words) to the same length in each batch. Without it, PyTorch would throw an error due to mismatched sequence lengths.\n",
        "\n"
      ],
      "metadata": {
        "id": "myEc5v0R4eW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**__init__**: Just stores the text data.\n",
        "\n",
        "**__len__**: Returns the number of words.\n",
        "\n",
        "**__getitem__**: This is the key. It converts each word into a tensor of character indices (ASCII values). For example, \"hello\" becomes tensor([104, 101, 108, 108, 111]). This numerical representation is what PyTorch can work with."
      ],
      "metadata": {
        "id": "bboLvOJF6ZkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torchvision.transforms for Data Augmentation**\n",
        "\n",
        "In Computer Vision (CV), data augmentation is crucial for increasing the diversity of the training data by applying random transformations to the input images. torchvision.transforms provides common image transformations."
      ],
      "metadata": {
        "id": "lkgvRTG16w2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Common Transformations in torchvision.transforms**\n",
        "\n",
        "**Resizing**: Resize images to a fixed size.\n",
        "Random Cropping: Randomly crop a portion of the image.\n",
        "\n",
        "**Random Horizontal Flip**: Flip the image randomly.\n",
        "\n",
        "**Normalization**: Normalize pixel values (mean and standard deviation)."
      ],
      "metadata": {
        "id": "totPAO-n635v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "AiiGWlO636oz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example image (use any own image here)\n",
        "img = Image.open('/content/cars.jpg')\n",
        "\n",
        "# Define a sequence of transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Apply transformations to the image\n",
        "img_transformed = transform(img)\n",
        "\n",
        "print(img_transformed.shape)     # Example output: torch.Size([3, 128, 128])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPl0FLYb7KYy",
        "outputId": "e05730f9-c583-4ff6-a4f0-a0bc62e86fbc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Data Augmentation with DataLoader**\n",
        "\n",
        "You can apply transformations directly to datasets during data loading by passing them to the transform argument of a Dataset. Here's how you'd do that:"
      ],
      "metadata": {
        "id": "gkNHL-Ru8xNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "# Define transformations to apply to each image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load a dataset (e.g., CIFAR-10) and apply transformations\n",
        "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader to batch and shuffle data\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Iterate through the DataLoader\n",
        "for images, labels in dataloader:\n",
        "  print(images.shape)   # Example output: torch.Size([32, 3, 128, 128])\n",
        "  break   # Just show one batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HmRWIdL8jmS",
        "outputId": "a329a602-5171-46aa-bdc8-b866afc16d81"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:01<00:00, 94.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "torch.Size([32, 3, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPU Acceleration in PyTorch**\n",
        "\n",
        "**to(device) â€“ Moving tensors/models to a specific device (GPU or CPU)**\n",
        "\n",
        "PyTorch allows you to run operations on either CPU or GPU. The GPU (Graphics Processing Unit) accelerates computations, especially for deep learning tasks. To move a tensor or model to the GPU, you use the to() method, specifying the device."
      ],
      "metadata": {
        "id": "ZLhQk_fpBkxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available, otherwise fall back to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create a tensor on the CPU\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "# Move tensor to GPU if available\n",
        "x = x.to(device)\n",
        "print(x.device)   # Output: cuda:0 if GPU is used, cpu if not"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHykZhD--DJI",
        "outputId": "429c5eb0-5846-4575-a36e-a3dc59351cd7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.device(\"cuda\")** refers to the **GPU**.\n",
        "\n",
        "**torch.device(\"cpu\")** refers to the **CPU**."
      ],
      "metadata": {
        "id": "E-IOIcsFCiYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cuda() & cpu() â€“ Moving tensors/models between CPU and GPU**\n",
        "\n",
        "You can also use **.cuda()** to move tensors to the GPU and **.cpu()** to move tensors back to the **CPU**. These methods are often used for compatibility between GPU and CPU operations (for example, when working with models that require CPU input for predictions)."
      ],
      "metadata": {
        "id": "TIku9qXiC846"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor on CPU\n",
        "tensor_cpu = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "# Move tensor to GPU\n",
        "tensor_gpu = tensor_cpu.cuda()\n",
        "print(tensor_gpu.device)   # Output: cuda:0\n",
        "\n",
        "# Move tensor back to CPU\n",
        "tensor_cpu_back = tensor_gpu.cpu()\n",
        "print(tensor_cpu_back.device)   # Output: cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42jTda3nCYLj",
        "outputId": "d9f2fb77-1bb4-4e7c-9a1e-7f63d2902094"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.cuda.amp â€“ Mixed Precision for faster training**\n",
        "\n",
        "Mixed Precision Training uses both 16-bit (half-precision) and 32-bit (single-precision) floating-point numbers during training. It can speed up training and reduce memory usage without sacrificing model accuracy. PyTorchâ€™s Automatic Mixed Precision (AMP) provides an easy way to enable mixed precision."
      ],
      "metadata": {
        "id": "-4nB99RADw47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Create a model and move it to GPU\n",
        "model = torch.nn.Linear(2, 2).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Initialize GradScaler for mixed precision\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Dummy data and target\n",
        "data = torch.randn(4, 2).cuda()\n",
        "target = torch.randn(4, 2).cuda()\n",
        "\n",
        "# Training loop with mixed precision\n",
        "for epoch in range(100):\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Automatically use mixed precision for the forward pass\n",
        "  with autocast():\n",
        "    output = model(data)\n",
        "    loss = torch.nn.functional.mse_loss(output, target)\n",
        "\n",
        "  # Scaler the loss and backward pass\n",
        "  scaler.scale(loss).backward()\n",
        "\n",
        "  # Step the optimizer with scaler to handle gradients\n",
        "  scaler.step(optimizer)\n",
        "  scaler.update()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VumUTmmZDUyH",
        "outputId": "cf734421-7611-4393-e839-982821eaa9e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-95b88821326d>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-8-95b88821326d>:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.6919735670089722\n",
            "Epoch 10, Loss: 1.1706044673919678\n",
            "Epoch 20, Loss: 0.8119431734085083\n",
            "Epoch 30, Loss: 0.5912909507751465\n",
            "Epoch 40, Loss: 0.4518594443798065\n",
            "Epoch 50, Loss: 0.36078619956970215\n",
            "Epoch 60, Loss: 0.2986535429954529\n",
            "Epoch 70, Loss: 0.2544364929199219\n",
            "Epoch 80, Loss: 0.2213820219039917\n",
            "Epoch 90, Loss: 0.19566218554973602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**autocast()**: This context manager automatically casts operations to mixed precision (16-bit).\n",
        "\n",
        "**GradScaler()**: Scales the gradients during backward pass to prevent underflow with mixed precision."
      ],
      "metadata": {
        "id": "IHZ7SuH7FdKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "**Model and Optimizer:**\n",
        "\n",
        "**model = torch.nn.Linear(2, 2).cuda()**:\n",
        "\n",
        "Creates a simple linear model and moves it to the GPU using .cuda(). Mixed precision training is most effective on GPUs.\n",
        "optimizer = torch.optim.SGD(...): Creates the optimizer (SGD in this case).\n",
        "\n",
        "**GradScaler**:\n",
        "\n",
        "**scaler = GradScaler()**:\n",
        "\n",
        "Initializes the GradScaler. The GradScaler is essential for mixed precision training. It handles the scaling of the loss to prevent underflow issues that can occur when using lower precision.\n",
        "Data and Target:\n",
        "\n",
        "**data = torch.randn(4, 2).cuda() and target = torch.randn(4, 2).cuda()**:\n",
        "\n",
        "Creates dummy input data and target values and moves them to the GPU.\n",
        "Training Loop: The training loop iterates for the specified number of epochs.\n",
        "\n",
        "**Autocast (Mixed Precision Forward Pass)**:\n",
        "\n",
        "**with autocast()**:\n",
        "\n",
        "This context manager enables mixed precision for the code within its block. PyTorch will automatically cast some operations to lower precision (typically FP16 or BF16) to speed up computation.\n",
        "\n",
        "output = model(data) and loss = ...:\n",
        "\n",
        "The forward pass and loss calculation are performed within the autocast context.\n",
        "Loss Scaling and Backward Pass:\n",
        "\n",
        "**scaler.scale(loss).backward()**:\n",
        "\n",
        "This is crucial for mixed precision. The scaler.scale(loss) part scales the loss up. This helps prevent gradients from becoming too small in lower precision, which can lead to underflow. The .backward() call then computes the gradients for the scaled loss.\n",
        "\n",
        "**Optimizer Step and Scaler Update:**\n",
        "\n",
        "**scaler.step(optimizer):**\n",
        "\n",
        "The scaler.step(optimizer) part is responsible for unscaling the gradients before the optimizer updates the parameters. It also skips updates for gradients that are too small.\n",
        "\n",
        "**scaler.update():** This updates the scaling factor used by the GradScaler for the next iteration. It monitors the gradients and adjusts the scaling factor dynamically to maintain numerical stability."
      ],
      "metadata": {
        "id": "VVog4qmaGjwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Deployment & Debugging**\n",
        "\n",
        "**torch.jit.trace â€“ JIT Compilation for optimizing models**\n",
        "\n",
        "The TorchScript is an intermediate representation of the model in PyTorch, and JIT (Just-In-Time) compilation allows you to optimize your model for deployment. torch.jit.trace is used to trace the operations of the model, which can be run more efficiently."
      ],
      "metadata": {
        "id": "RenVyOvEH2Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define a simple model\n",
        "class SimpleModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = torch.nn.Linear(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate model and move it to GPU\n",
        "model = SimpleModel().cuda()\n",
        "\n",
        "# Create example input (matching the model input size)\n",
        "input_data = torch.randn(1, 2).cuda()\n",
        "\n",
        "# Trace the model with the input data\n",
        "traced_model = torch.jit.trace(model, input_data)\n",
        "\n",
        "# Save the traced model\n",
        "traced_model.save(\"traced_model.pt\")\n",
        "\n",
        "# Load and run the traced model for inference\n",
        "loaded_model = torch.jit.load(\"traced_model.pt\")\n",
        "output = loaded_model(input_data)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY1gR7ZyD7Yz",
        "outputId": "b51d6e97-53d0-4100-d1a1-dd6f3bb2fefb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.7818, -0.0381]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.jit.trace(): Traces the model to create a TorchScript version of it, which can be optimized and deployed independently of Python.\n",
        "The model can then be saved, loaded, and run on any platform that supports PyTorch, without needing a Python interpreter."
      ],
      "metadata": {
        "id": "7tEtLDeuIGGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.profiler â€“ Performance Analysis for Debugging and Optimization**\n",
        "\n",
        "To analyze and optimize the performance of your PyTorch model, you can use torch.profiler. It helps you understand where the bottlenecks are during training, so you can focus on improving the critical parts."
      ],
      "metadata": {
        "id": "2Mi6F3AoIMGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.profiler\n",
        "\n",
        "# Define a simple model\n",
        "class SimpleModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = torch.nn.Linear(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Instantiate model\n",
        "model = SimpleModel().cuda()\n",
        "\n",
        "# Create example input\n",
        "input_data = torch.randn(1, 2).cuda()\n",
        "\n",
        "# Profile the training process\n",
        "with torch.profiler.profile(\n",
        "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "    record_shapes=True,\n",
        "    profile_memory=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    for _ in range(100):\n",
        "        model(input_data)  # Forward pass\n",
        "\n",
        "# Print profiling results\n",
        "prof.export_chrome_trace(\"model_profile.json\")  # Export the profile to Chrome Trace format\n",
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPX1gh-CH_mr",
        "outputId": "6a3bc7ee-ec65-4d03-e877-0b16ebf5ae50"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::linear        18.96%       1.541ms        99.74%       8.109ms      81.090us       0.000us         0.00%     427.412us       4.274us           0 b           0 b      50.00 Kb           0 b           100  \n",
            "                                            aten::addmm        58.38%       4.747ms        69.29%       5.633ms      56.333us     427.412us       100.00%     427.412us       4.274us           0 b           0 b      50.00 Kb      50.00 Kb           100  \n",
            "                                                aten::t         5.32%     432.628us        11.49%     934.528us       9.345us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           100  \n",
            "                                       cudaLaunchKernel        10.91%     886.709us        10.91%     886.709us       8.867us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           100  \n",
            "                                        aten::transpose         4.67%     380.017us         6.17%     501.900us       5.019us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           100  \n",
            "                                       aten::as_strided         1.50%     121.883us         1.50%     121.883us       1.219us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           100  \n",
            "                                  cudaDeviceSynchronize         0.26%      21.115us         0.26%      21.115us      21.115us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
            "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     427.412us       100.00%     427.412us       4.274us           0 b           0 b           0 b           0 b           100  \n",
            "                                               [memory]         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     -50.00 Kb     -50.00 Kb           100  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 8.130ms\n",
            "Self CUDA time total: 427.412us\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**torch.profiler.profile()**: This context manager profiles the execution of your model, both on CPU and GPU.\n",
        "\n",
        "**key_averages().table()**: Displays the profiling results in a table, showing which operations took the most time."
      ],
      "metadata": {
        "id": "r_qgha0fIXbM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8PMttUpPIPPq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}